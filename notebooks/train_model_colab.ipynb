{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPFPVRpf+5Hlm3QxuVGMwMX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Lucalangella/tennis-court-classifier/blob/main/notebooks/train_model_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kuOuleCdJbXO"
      },
      "outputs": [],
      "source": [
        "# --- BLOCK 1: INSTALL DEPENDENCIES & FIX RUNTIME ---\n",
        "import os\n",
        "import time\n",
        "\n",
        "print(\"1. Installing compatible libraries...\")\n",
        "# limit output with -qq\n",
        "!pip install -Uqq \"numpy<2\" fastai==2.7.14 fastprogress==1.0.3 coremltools\n",
        "\n",
        "# Check if we need to restart\n",
        "try:\n",
        "    import numpy\n",
        "    # If numpy is loaded and is version 2+, we MUST restart\n",
        "    if numpy.__version__.startswith('2'):\n",
        "        print(\"⚠️ Incompatible NumPy detected. Restarting runtime automatically...\")\n",
        "        print(\"⏳ Please wait for the runtime to reconnect, then RUN BLOCK 2.\")\n",
        "        time.sleep(1) # Give the print statement time to show\n",
        "        os.kill(os.getpid(), 9)\n",
        "    else:\n",
        "        print(\"✅ Environment is ready! You can proceed to Block 2.\")\n",
        "except ImportError:\n",
        "    print(\"✅ Environment is ready! You can proceed to Block 2.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- BLOCK 2: CLONE DATA & SETUP PATHS ---\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "from fastai.vision.all import *\n",
        "\n",
        "# 1. Clean up old runs (to ensure we pull fresh data)\n",
        "repo_name = \"tennis-court-classifier\"\n",
        "if os.path.exists(repo_name):\n",
        "    shutil.rmtree(repo_name)\n",
        "\n",
        "# 2. Clone the repository\n",
        "user = \"LucaLangella\"\n",
        "print(f\"Cloning repository from user: {user}...\")\n",
        "!git clone https://github.com/{user}/{repo_name}.git\n",
        "\n",
        "# 3. Define path\n",
        "path = Path(f'./{repo_name}/tennis_courts')\n",
        "\n",
        "# 4. Verify\n",
        "if path.exists():\n",
        "    print(f\"✅ Setup complete. Data path set to: {path}\")\n",
        "    print(f\"   Found {len(get_image_files(path))} images.\")\n",
        "else:\n",
        "    print(\"❌ Error: Data folder not found. Check the repository name.\")"
      ],
      "metadata": {
        "id": "G-ifraTbScJH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- BLOCK 3: VERSION VERIFICATION ---\n",
        "import numpy\n",
        "import fastai\n",
        "import coremltools\n",
        "\n",
        "print(f\"✅ NumPy Version: {numpy.__version__}\")\n",
        "# Target Version: 1.26.x (Must be below 2.0)\n",
        "\n",
        "print(f\"✅ FastAI Version: {fastai.__version__}\")\n",
        "# Target Version: 2.7.14\n",
        "\n",
        "print(\"Refinding your data...\")\n",
        "from fastai.vision.all import *\n",
        "path = Path('./tennis-court-classifier/tennis_courts')\n",
        "\n",
        "if path.exists():\n",
        "    print(f\"✅ Data still there! Found {len(get_image_files(path))} images.\")\n",
        "else:\n",
        "    print(\"⚠️ Data was wiped by the restart. Please Run 'Block 1' again (The Setup Block).\")"
      ],
      "metadata": {
        "id": "geT66JjrNAYQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- BLOCK 4: DATASET AUGMENTATION (UNKNOWN CLASS) ---\n",
        "from fastai.vision.all import *\n",
        "\n",
        "# 1. Create the directory for the 'unknown' category\n",
        "unknown_folder = path / 'unknown'\n",
        "unknown_folder.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# 2. Check folder content and populate with noise data if empty\n",
        "if len(get_image_files(unknown_folder)) < 10:\n",
        "    print(\"⚠️ 'Unknown' folder is empty. Downloading junk images...\")\n",
        "\n",
        "    # Download Imagenette (a dataset of random objects)\n",
        "    path_random = untar_data(URLs.IMAGENETTE_160)\n",
        "\n",
        "    # Transfer 50 random images to serve as negative examples\n",
        "    random_files = get_image_files(path_random/'train')[:50]\n",
        "    for i, file in enumerate(random_files):\n",
        "        shutil.copy(file, unknown_folder / f'random_{i}.jpg')\n",
        "\n",
        "    print(f\"✅ Added {len(random_files)} images to 'unknown' folder.\")\n",
        "else:\n",
        "    print(f\"✅ 'Unknown' folder already has {len(get_image_files(unknown_folder))} images.\")"
      ],
      "metadata": {
        "id": "rfPSqJvs8PrX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- BLOCK 5: MODEL TRAINING ---\n",
        "# 1. Initialize DataLoaders with resizing and augmentation\n",
        "dls = ImageDataLoaders.from_folder(\n",
        "    path,\n",
        "    valid_pct=0.2,\n",
        "    seed=42,\n",
        "    item_tfms=Resize(224, method='squish'),\n",
        "    batch_tfms=aug_transforms(mult=1.5),\n",
        "    bs=32,\n",
        "    num_workers=2\n",
        ")\n",
        "\n",
        "print(f\"Classes found: {dls.vocab}\")\n",
        "# Verification: Expecting ['clay', 'grass', 'hard', 'unknown']\n",
        "\n",
        "# 2. Instantiate the Convolutional Neural Network (ResNet18)\n",
        "learn = vision_learner(dls, resnet18, metrics=error_rate)\n",
        "\n",
        "# 3. Execute training loop\n",
        "print(\"\\nStarting training...\")\n",
        "# Note: Ensure this line executes without indentation errors\n",
        "learn.fine_tune(4)\n",
        "\n",
        "print(\"✅ Training complete!\")"
      ],
      "metadata": {
        "id": "eKgqmkSUKA80"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- BLOCK 6: PREDICTION VISUALIZATION ---\n",
        "# Display sample predictions from the validation set\n",
        "# Format: Label = Actual Class / Prediction = Model Output\n",
        "print(\"Displaying predictions (Green = Correct, Red = Mistake)...\")\n",
        "learn.show_results(max_n=6, figsize=(7, 8))"
      ],
      "metadata": {
        "id": "z_KInbTJNnhu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- BLOCK 7: EXTERNAL VALIDATION (TEST SET) ---\n",
        "import os\n",
        "from fastai.vision.all import *\n",
        "\n",
        "# 1. Define path to the separate test dataset\n",
        "repo_name = \"tennis-court-classifier\"\n",
        "test_path = Path(f'./{repo_name}/test_images_set')\n",
        "\n",
        "# 2. Verify existence of test files\n",
        "if test_path.exists():\n",
        "    files_found = get_image_files(test_path)\n",
        "    print(f\"✅ FOUND TEST SET: {len(files_found)} images\")\n",
        "\n",
        "    # 3. Generate predictions on test data\n",
        "    print(\"\\n--- GRADING YOUR MODEL ---\")\n",
        "\n",
        "    test_dl = learn.dls.test_dl(files_found)\n",
        "    preds, _ = learn.get_preds(dl=test_dl)\n",
        "\n",
        "    for i, file in enumerate(files_found):\n",
        "        predicted_index = preds[i].argmax()\n",
        "        predicted_label = dls.vocab[predicted_index]\n",
        "        confidence = preds[i][predicted_index] * 100\n",
        "\n",
        "        flag = \"⚠️\" if confidence < 60 else \"✅\"\n",
        "        print(f\"{flag} File: {file.name.ljust(20)} --> Predicted: {predicted_label.upper().ljust(10)} ({confidence:.1f}%)\")\n",
        "\n",
        "else:\n",
        "    print(f\"❌ ERROR: Could not find folder at {test_path}\")\n",
        "    print(\"You may need to run 'Block 1' again to re-download the updated repo.\")"
      ],
      "metadata": {
        "id": "BRy_LYIiNvAX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- BLOCK 8: ERROR ANALYSIS ---\n",
        "from fastai.vision.all import *\n",
        "\n",
        "# 1. Initialize Classification Interpretation\n",
        "interp = ClassificationInterpretation.from_learner(learn)\n",
        "\n",
        "# 2. Generate and display the Confusion Matrix\n",
        "print(\"Generating Confusion Matrix...\")\n",
        "interp.plot_confusion_matrix(figsize=(7, 7))\n",
        "\n",
        "# 3. Visualize the top 5 most confident incorrect predictions\n",
        "print(\"\\nTop 5 Biggest Mistakes:\")\n",
        "interp.plot_top_losses(5, nrows=1)"
      ],
      "metadata": {
        "id": "PK-V4GCHNzeT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- BLOCK 9: MANUAL INFERENCE WIDGET ---\n",
        "import ipywidgets as widgets\n",
        "from PIL import Image\n",
        "import io\n",
        "\n",
        "# 1. Initialize file upload widget and output display\n",
        "btn_upload = widgets.FileUpload()\n",
        "out_pl = widgets.Output()\n",
        "lbl_pred = widgets.Label()\n",
        "\n",
        "def on_data_change(change):\n",
        "    lbl_pred.value = ''\n",
        "\n",
        "    # Process the uploaded image\n",
        "    img = PILImage.create(btn_upload.data[-1])\n",
        "    out_pl.clear_output()\n",
        "    with out_pl: display(img.to_thumb(224,224))\n",
        "\n",
        "    # Run inference\n",
        "    pred, pred_idx, probs = learn.predict(img)\n",
        "\n",
        "    # Update label with result\n",
        "    lbl_pred.value = f'PREDICTION: {pred.upper()} (Confidence: {probs[pred_idx]*100:.2f}%)'\n",
        "\n",
        "# Bind the event handler\n",
        "btn_upload.observe(on_data_change, names=['data'])\n",
        "\n",
        "# Render the UI\n",
        "print(\"Click 'Upload' to test a new image:\")\n",
        "display(widgets.VBox([btn_upload, out_pl, lbl_pred]))"
      ],
      "metadata": {
        "id": "W9M6NIjuKDFh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- BLOCK 10: COREML EXPORT ---\n",
        "import coremltools as ct\n",
        "import torch\n",
        "from google.colab import files\n",
        "\n",
        "print(\"Converting model to CoreML...\")\n",
        "\n",
        "# 1. Switch model to CPU evaluation mode for tracing\n",
        "model = learn.model.eval().cpu()\n",
        "dummy_input = torch.rand(1, 3, 224, 224)\n",
        "traced_model = torch.jit.trace(model, dummy_input)\n",
        "\n",
        "# 2. Convert PyTorch trace to CoreML format with ImageNet normalization\n",
        "mlmodel = ct.convert(\n",
        "    traced_model,\n",
        "    inputs=[ct.ImageType(\n",
        "        name=\"image\",\n",
        "        shape=dummy_input.shape,\n",
        "        scale=1/255.0,\n",
        "        bias=[-0.485/0.229, -0.456/0.224, -0.406/0.225]\n",
        "    )],\n",
        "    classifier_config=ct.ClassifierConfig(list(dls.vocab))\n",
        ")\n",
        "\n",
        "# 3. Add metadata and compress the artifact\n",
        "mlmodel.short_description = \"Tennis Court Classifier\"\n",
        "mlmodel.author = \"Luca\"\n",
        "save_name = \"TennisClassifier_Colab.mlpackage\"\n",
        "mlmodel.save(save_name)\n",
        "\n",
        "print(\"✅ Model saved! Zipping...\")\n",
        "zip_name = \"TennisClassifier_Final\"\n",
        "shutil.make_archive(zip_name, 'zip', save_name)\n",
        "\n",
        "# 4. Trigger file download\n",
        "print(\"⬇️ Downloading file...\")\n",
        "files.download(f\"{zip_name}.zip\")"
      ],
      "metadata": {
        "id": "TwkNGg1rQ06m"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}